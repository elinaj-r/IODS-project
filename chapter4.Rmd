# RStudio Exercise 4: Clustering and classification

This week's exercise will use a data from MASS package.

## Reading data and exploring data

```{r}
# Elina Järvelä-Reijonen
# 25.11.2019


# Loading the data
library(MASS)
data('Boston')

# Exploring the structure and dimensions of the data
str(Boston)
dim(Boston)
```

**Interpretation:** The dataset includes 506 observations and 14 variables. All the variables are numerical. The data describes housing values in suburbs of Boston, such as average number of rooms per dwelling and nitrogen oxides concentration. The full description can be found here: <https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html>.

```{r}
# A graphical overview of the data and summaries of the variables in the data

pairs(Boston)
summary(Boston)
```

**Interpretation:** Based on the plot, some variables seem to have linear association (for example an inverse association between lstat and medv, i.e. percentage of lower status of the population and median value of owner-occupied homes). Per capita crime rate (crim) has a large range (0 - 89).

```{r}
# Drawing a correlations plot
# access the 'corrplot' and 'tidyr' packages
library(corrplot)
library(tidyr)


# calculate the correlation matrix and round it (use two digits)
cor_matrix<-cor(Boston)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```
**Interpretation:** The correlations plot shows several strong correlations between the variables. A bigger dot describes a bigger/stronger correlation. Blue dots represent postivie correlation and red dots negative correlation. For example, indus and nox has a strong positive correlation (i.e., the more non-retail business acres per town, the higher is the nitrogen oxides concentration).


## Standardizing the variables and categorizing the crime rate

```{r}
# Let's standardize the variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# the variables are now centered so that the mean is 0

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# Let's use the bins to create a categorical variable
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE)

# look at the table of the new factor crime
table(crime)

# create a categorical variable 'crime' and set the labels
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# Let's remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)


```

## Creating the train and test datasets

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

## Linear discriminant analysis (LDA)
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

```
The Linear Discriminant 1 explains most, 95%, of the between-group variance. Accessibility to radial highways (rad) seems to be the most influential variable in the LD1.

```{r}
# drawing the (bi)plot
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)


```

## Predicting with the LDA model

The crime categories were already saved previously to correct_classes and the crime categories were removed from the data set (in the Creating the train and test datasets -section).

```{r}
# predict classes with LDA model on the test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results with the crime categories from the test set
table(correct = correct_classes, predicted = lda.pred$class)


```

**Interpretation:** The model predicts high crime rate category well. Also the med_high and med_low are rather accurately predicted. However, the model is not very good to predict low crime rate accurately.

## K-means clustering
```{r}
# Reloading the data
library(MASS)
data('Boston')

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)


```
```{r}
# calculating euclidean distances
dist_eu <- dist(boston_scaled)
summary(dist_eu)

# calculating manhattan distances
dist_man <- dist(boston_scaled, method = "manhattan")
summary(dist_man)

```
```{r}
# k-means clustering, let's try with 3 clusters
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled [1:5], col = km$cluster)
pairs(boston_scaled [6:10], col = km$cluster)
pairs(boston_scaled [11:14], col = km$cluster)
```
```{r}
# let's try to find the best number for clusters, maximum 10
set.seed(123)
k_max <- 10

# calculate the total within sum of squares (WCSS)
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```


**Interpretation:** When the number of clusters and the total WCSS are plotted, the optimal number of clusters is when the total WCSS drops radically.Thus, it could be interpreted that the optimal number of clusters in this case would be 2 or 7. I think it would be easier, if the number of clusters is low, so, let's take 2.

```{r}

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the scaled Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)


```